# LLM-jh-GPT
A <ins>Generative Pre-trained Transformer</ins> (GPT) is a type of artificial intelligence that understands and generates human-like text. We will be using the <ins>PyTorch.nn (Neural network) library</ins> which houses transformer architecture. The goal of jhGPT is to output linguistic text similar to the capabilities of humans. Ultimately, we want the model to produce undifferentiable text (compared to a human). The model will have a range of languages, initially starting with English, and then moving forward to other languages.


## Transformer Architecture used in jhGPT
<p align="center">
  <img src="https://github.com/Hy8012/LLM-jh-GPT/blob/main/md_files/Transformer.png?raw=true" width="400" height="675"/>
</p>

The picture above is a basic Transformer Architecture as described in <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"><i>Attention Is All You Need</i></a>.
